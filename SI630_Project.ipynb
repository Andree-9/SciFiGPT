{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f065da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded5e053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiayuu/.local/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load the dataset\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='scifi.txt',\n",
    "    block_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c4497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243564 27063\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Split the dataset into training and validation subsets\n",
    "train_size = int(len(dataset) * 0.9)  # Use 90% of the data for training\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "print(train_size, val_size)\n",
    "# Define the data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd105465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=50,\n",
    "    per_device_eval_batch_size=50,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1000,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46aa9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1084' max='542' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [542/542 46:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity without Finetuning: 62.86\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_loss = trainer.evaluate(val_dataset)['eval_loss']\n",
    "perplexity = math.exp(eval_loss)\n",
    "print(f'Perplexity without Finetuning: {perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22689c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiayuu/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14616' max='14616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14616/14616 2:15:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.906300</td>\n",
       "      <td>3.801984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>3.759054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.766500</td>\n",
       "      <td>3.746405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='542' max='542' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [542/542 01:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after Finetuning: 42.37\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the perplexity on the test set\n",
    "eval_loss = trainer.evaluate(val_dataset)['eval_loss']\n",
    "perplexity = math.exp(eval_loss)\n",
    "print(f'Perplexity after Finetuning: {perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20ac851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After a catastrophic event leaves Earth uninhabitable, the last remaining humans are forced to live on a space station orbiting the planet. But when a group of rebels discover a way to potentially save Earth, they must decide whether to risk their lives for a chance at redemption or continue living in their artificial home. The book is one of those rare gems that have never been published before -- and it's an important addition to any science fiction library! (Publ., #) by Robert Silverberg  Theodore Sturgeon Isaac Asimov Harlan Ellison Damon Knight Frank Herbert Locus Press Edmond Hamilton Fritz Leiber Jack Vance Lester del Rey Frederik Pohl James Blish Roger Zelazny Larry Niven A S E J H OW was born into slavery during World War II. He came from a family of wealthy farmers who had bought up all the land under the New York Stock Exchange as slaves; but his parents were able only to sell them out because he could not find enough money to buy back what he needed. When he reached college, he spent most of his time working with the New York Stock Exchange. In school, he learned how to trade paper currency so that people would think about him. His interest in trading became more than just a hobby; he found himself involved with other men and women whose ideas might help others gain knowledge. After graduation, he joined the American Association of Certified Professional Engineers, which made training possible for both professional engineers and mechanics. And after three years of study, he decided\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = \"The year is 2050 and humanity has just discovered a new planet capable of sustaining life. A team of scientists and explorers are sent to investigate. What they find will change the course of human history forever.\"\n",
    "prompt_2 = \"In a world where technology has advanced to the point of being able to upload human consciousness into a virtual reality, one man must navigate this new world and the dangers that come with it. But when he discovers a dark secret about the virtual world, he must decide whether to stay in blissful ignorance or risk everything to expose the truth.\"\n",
    "prompt_3 = \"After a catastrophic event leaves Earth uninhabitable, the last remaining humans are forced to live on a space station orbiting the planet. But when a group of rebels discover a way to potentially save Earth, they must decide whether to risk their lives for a chance at redemption or continue living in their artificial home.\"\n",
    "\n",
    "generated_text = model.generate(\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt_3)).unsqueeze(0).to('cuda'),\n",
    "    max_length=300,\n",
    "    temperature=0.5,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(generated_text[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774e0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('scifigpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f8ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
